# Auto-generated pipeline config
_generated:
  by: generate_sae_config.py
  dataset: ernanhughes/openorca-1k-short
  date: '2025-03-28 11:25:57'
  model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
batch_size: 1
database:
  uri: postgresql://reasoning:reasoning@localhost:5432/reasoning
model:
  layer_index: 12
  max_seq_len: 64
  name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
  pad_to_max: true
pipeline:
  activation_extraction:
    device: cuda
    enabled: true
    enabled_reason: Required to get activations.
    reason: Model forward pass benefits from GPU.
  activation_storage:
    device: cpu
    enabled: true
    enabled_reason: We want to persist extracted data.
    reason: Storage can run on CPU safely.
  feature_scoring:
    device: cpu
    enabled: true
    enabled_reason: Compute ReasonScore for activations.
    reason: Sparse vector scoring is CPU-safe.
  sae_steering:
    device: cpu
    enabled: true
    enabled_reason: SAE steering.
    reason: To much memory to run on GPU.
  sae_training:
    device: cpu
    enabled: true
    enabled_reason: Train a new SAE for this setup.
    reason: Efficient on CPU unless input_dim is huge.
  tokenization:
    device: cpu
    enabled: true
    enabled_reason: Always needed.
    reason: Tokenization is lightweight and runs well on CPU.
prompts:
  dataset: ernanhughes/openorca-1k-short
  limit: 1000
  split: train
  text_column: question
reason_score:
  keyword_file: configs/reasoning_keywords.txt
  mode: keywords
  token_file: configs/reasoning_token_ids.txt
  top_k_normalization:
    enabled: false
    k: 10
sparse_autoencoder:
  config: configs/sae\tinyllama-tinyllama-1.1b-chat-v1.0_ernanhughes-openorca-1k-short_layer12.yaml
  path: sae_models/tinyllama-tinyllama-1.1b-chat-v1.0_ernanhughes-openorca-1k-short_layer12
  train_if_missing: true
